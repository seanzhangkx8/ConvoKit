{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6248e97c",
   "metadata": {},
   "source": [
    "# Validation Baselines\n",
    "\n",
    "This notebook implements and evaluates baseline similarity measures used for comparison with our ConDynS measure, where the results are demonstrated in the other notebook. It computes metrics such as SBERT cosine similarity, BERTScore, and naive LLM-prompted similarity on both transcript and SCD representations of conversations. These baselines serve as reference points in the validation experiment, allowing us to assess the unique contribution of ConDynS in capturing conversational dynamics beyond topic or surface-level features. Detailed discussion can be found in Section 5 of our [paper: A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cd50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from convokit import Corpus, download\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from convokit.convo_similarity.utils import get_human_summary_pair_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "corpus.print_summary_stats()\n",
    "\n",
    "human_pair_lst = get_human_summary_pair_lst(corpus)\n",
    "convo_pairs = human_pair_lst + [(j, i) for i, j in human_pair_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEFACTS_DIR = \"./artefacts/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be2341",
   "metadata": {},
   "source": [
    "# Running All Baseline Methods\n",
    "\n",
    "The following script runs all baseline similarity metrics (e.g., SBERT cosine similarity, BERTScore, naive prompting) on the provided conversation pairs, with different input types (raw transcript of the conversation, or its SCD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df112e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations.json\", \"r\") as f:\n",
    "    transcript_simulations = json.load(f)\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations_topic_shuffled.json\", \"r\") as f:\n",
    "    transcript_simulations_topic_shuffled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calling GPT to make naive comparisons, run with caution.\n",
    "from convokit.convo_similarity.utils import format_transcript_from_convokit, get_human_summary\n",
    "from convokit.convo_similarity.baseline import ConDynSBaselines\n",
    "from convokit.genai.genai_config import GenAIConfigManager\n",
    "\n",
    "config = GenAIConfigManager() ## make sure to set your own config if this is never set before\n",
    "baselines = ConDynSBaselines(model_provider=\"gpt\", config=config)\n",
    "\n",
    "self_results = {}\n",
    "self_scores = []\n",
    "for convo_id1, convo_id2 in tqdm(convo_pairs, desc=\"Calculating self sim similarity\"):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id1))\n",
    "    transcript2 = transcript_simulations[convo_id1]['generated_transcript']\n",
    "\n",
    "    scd1 = get_human_summary(corpus, convo_id1)['summary_text']\n",
    "    scd2 = transcript_simulations[convo_id1]['summary']['summary_text']\n",
    "\n",
    "    transcript_bertscore = baselines.get_bertscore(transcript1, transcript2)['f1'][0]\n",
    "    transcript_cos_sim = baselines.get_cosine_similarity(transcript1, transcript2)\n",
    "    transcript_naive_gpt, _ = baselines.get_naive_gpt_compare_score_Transcripts(transcript1, transcript2)\n",
    "\n",
    "    scd_bertscore = baselines.get_bertscore(scd1, scd2)['f1'][0]\n",
    "    scd_cos_sim = baselines.get_cosine_similarity(scd1, scd2)\n",
    "    scd_naive_gpt, _ = baselines.get_naive_gpt_compare_score_SCDs(scd1, scd2)\n",
    "\n",
    "    results = {\"transcript_bertscore\" : transcript_bertscore,\n",
    "              \"transcript_cos_sim\" : transcript_cos_sim,\n",
    "              \"transcript_naive_gpt\" : transcript_naive_gpt,\n",
    "              \"scd_bertscore\" : scd_bertscore,\n",
    "              \"scd_cos_sim\" : scd_cos_sim,\n",
    "              \"scd_naive_gpt\" : scd_naive_gpt}\n",
    "    self_results[str(convo_id1)] = results\n",
    "    self_scores.append(results)\n",
    "\n",
    "pair_results = {}\n",
    "pair_scores = []\n",
    "for convo_id1, convo_id2 in tqdm(convo_pairs, desc=\"Calculating pair sim similarity\"):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id1))\n",
    "    transcript2 = transcript_simulations[convo_id2]['generated_transcript']\n",
    "    \n",
    "    scd1 = get_human_summary(corpus, convo_id1)['summary_text']\n",
    "    scd2 = transcript_simulations[convo_id2]['summary']['summary_text']\n",
    "\n",
    "    transcript_bertscore = baselines.get_bertscore(transcript1, transcript2)['f1'][0]\n",
    "    transcript_cos_sim = baselines.get_cosine_similarity(transcript1, transcript2)\n",
    "    transcript_naive_gpt, _ = baselines.get_naive_gpt_compare_score_Transcripts(transcript1, transcript2)\n",
    "\n",
    "    scd_bertscore = baselines.get_bertscore(scd1, scd2)['f1'][0]\n",
    "    scd_cos_sim = baselines.get_cosine_similarity(scd1, scd2)\n",
    "    scd_naive_gpt, _ = baselines.get_naive_gpt_compare_score_SCDs(scd1, scd2)\n",
    "\n",
    "    results = {\"transcript_bertscore\" : transcript_bertscore,\n",
    "              \"transcript_cos_sim\" : transcript_cos_sim,\n",
    "              \"transcript_naive_gpt\" : transcript_naive_gpt,\n",
    "              \"scd_bertscore\" : scd_bertscore,\n",
    "              \"scd_cos_sim\" : scd_cos_sim,\n",
    "              \"scd_naive_gpt\" : scd_naive_gpt}\n",
    "    \n",
    "    pair_results[str(convo_id2)] = results\n",
    "    pair_scores.append(results)\n",
    "\n",
    "\n",
    "all_baseline_scores = {\"self_results\" : self_results,\n",
    "                       \"self_scores\" : self_scores,\n",
    "                       \"pair_results\" : pair_results,\n",
    "                       \"pair_scores\" : pair_scores}\n",
    "\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/baseline/baseline_results.json\", \"w\") as f:\n",
    "    json.dump(all_baseline_scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ARTEFACTS_DIR + \"validation_gpt/baseline/baseline_results.json\", \"r\") as f:\n",
    "    all_baseline_scores = json.load(f)\n",
    "\n",
    "self_scores = all_baseline_scores[\"self_scores\"]\n",
    "pair_scores = all_baseline_scores[\"pair_scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6627a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_scores(self_scores, pair_scores, input_type, score_method):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        input_type: transcript, scd\n",
    "        score_method: cos_sim, bertscore, naive_gpt\n",
    "    \"\"\"\n",
    "    accuracy = []\n",
    "    self_raw_scores = [x[f\"{input_type}_{score_method}\"] for x in self_scores]\n",
    "    pair_raw_scores = [x[f\"{input_type}_{score_method}\"] for x in pair_scores]\n",
    "    accuracy = [x > y for x, y in zip(self_raw_scores, pair_raw_scores)]\n",
    "\n",
    "    print(sum(accuracy) / len(accuracy))\n",
    "    print(np.mean(self_raw_scores))\n",
    "    print(np.mean(pair_raw_scores))\n",
    "    print(stats.wilcoxon(self_raw_scores, pair_raw_scores))\n",
    "\n",
    "    plt.hist(self_raw_scores, alpha = 0.6, label = \"self simulation\")\n",
    "    plt.hist(pair_raw_scores, alpha = 0.6, label = \"pair simulation\")\n",
    "    plt.xlabel(\"number of conversations\")\n",
    "    plt.ylabel(\"similarity scores\")\n",
    "    plt.title(f\"Baseline ({score_method}) Score distribution of sim vs sim ({input_type})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def get_baseline_acc(self_scores, pair_scores, input_type, score_method):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        input_type: transcript, scd\n",
    "        score_method: cos_sim, bertscore, naive_gpt\n",
    "    \"\"\"\n",
    "    accuracy = []\n",
    "    self_raw_scores = [x[f\"{input_type}_{score_method}\"] for x in self_scores]\n",
    "    pair_raw_scores = [x[f\"{input_type}_{score_method}\"] for x in pair_scores]\n",
    "    accuracy = [x > y for x, y in zip(self_raw_scores, pair_raw_scores)]\n",
    "\n",
    "    return sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bea43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Outputting the results from all baseline methods ###\n",
    "input_types = [\"transcript\", \"scd\"]\n",
    "score_methods = [\"cos_sim\", \"bertscore\", \"naive_gpt\"]\n",
    "\n",
    "for score_method in score_methods:\n",
    "    for input_type in input_types:\n",
    "        acc = get_baseline_acc(self_scores, pair_scores, input_type, score_method)\n",
    "        print(f\"###### {score_method} + {input_type} ######\")\n",
    "        print(f\"Acc: {acc:.4f}\\n\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
