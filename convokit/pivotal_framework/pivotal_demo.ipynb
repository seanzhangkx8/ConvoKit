{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed573d1",
   "metadata": {},
   "source": [
    "# Pivotal Moments Demo\n",
    "This notebook demonstrates our pivotal-moments framework introduced in this paper: Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations. In the paper, we consider a moment *pivotal* if the next response is expected to have a large impact on the conversationâ€™s eventual outcome.\n",
    "\n",
    "Here, we demo our framework on online conversations in the CGA-CMV (Conversations Gone Awry-Change My View) setting, consisting of conversations that may derail into personal attacks (Chang and Danescu-Niculescu-Mizil, 2019). We provide an initial exploration into identifying pivotal moments with respect to the outcome of conversation derailment. Furthermore, we release the demo to encourage future work and applications in other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a27c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/CornellNLP/ConvoKit.git\n",
    "# !pip install -q convokit\n",
    "\n",
    "# Do this only in Colab notebooks!\n",
    "# !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "# !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "# !pip install transformers==4.51.3\n",
    "# !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2256c58",
   "metadata": {},
   "source": [
    "We first import all the necessary packages and modules that will be used in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603194f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from convokit import Corpus, download\n",
    "from convokit.pivotal_framework.pivotal import PivotalMomentMeasure\n",
    "from convokit.utterance_simulator.unslothUtteranceSimulatorModel import UnslothUtteranceSimulatorModel\n",
    "\n",
    "from convokit.forecaster import TransformerEncoderModel, TransformerForecasterConfig\n",
    "\n",
    "import random\n",
    "from functools import partial\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14049f66",
   "metadata": {},
   "source": [
    "We then download the `conversations-gone-awry-cmv-corpus` corpus that we will be using throughout the demo. If you already have the corpus saved locally, you could specify the path to load the corpus from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02abbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "# If you have the corpus saved locally, load it as follows:\n",
    "# corpus = Corpus(\"<PATH_TO_CORPUS>\")\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78ea30",
   "metadata": {},
   "source": [
    "The `conversations-gone-awry-cmv-corpus` corpus consists of Reddit conversations that may derail into personal attacks and conversations that remain calm.\n",
    "\n",
    "The conversations in the corpus are paired based on the length (number of utterances) of the conversation, where each pair consists of one *derailed* conversation and one *calm* conversation (indicated by the `has_removed_comment` metadata field). In our demo, we will select conversations to train, validate, and test our framework. To maintain this pairing in the data selection, we first create a set of conversation ids, consisting of *one* conversation id from each pair, which we will use to sample from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ids = set()\n",
    "for i, convo in enumerate(corpus.iter_conversations()):\n",
    "    pair_id = convo.meta['pair_id']\n",
    "    if convo.id in pair_ids:\n",
    "        continue\n",
    "    pair_ids.add(pair_id)\n",
    "\n",
    "pair_ids = list(pair_ids)\n",
    "print(len(pair_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d48c7d",
   "metadata": {},
   "source": [
    "Our pivotal-moments framework consists of two main components: (1) *simulator model* to simulate potential next responses and (2) *forecaster model* to predict the likelihood of the outcome based on these potential responses. \n",
    "\n",
    "To train, validate, and test these components, we sample pairs of conversations from the corpus, where each pair consists of one *derailed* conversation and one *calm* conversation. Therefore, by selecting `x` pairs, we are selecting `x * 2` conversations to be included in a given set.\n",
    "\n",
    "Here, we use a 80/10/10 train/val/test split for our forecaster model and a 90/10 train/val split for fine-tuning our simulator model. \n",
    "\n",
    "We also sample conversations to be included in our analysis set. \n",
    "\n",
    "Alternatively, our framework also supports using pre-existing trained models for simulation/forecasting, so you can skip this data setup if you go with this route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_pair_ids = random.sample(pair_ids, 500)\n",
    "random.shuffle(forecast_pair_ids)\n",
    "\n",
    "size = len(forecast_pair_ids)\n",
    "forecast_pair_train_ids = forecast_pair_ids[:int(0.8*size)]\n",
    "forecast_pair_val_ids = forecast_pair_ids[int(0.8*size): int(0.9*size)]\n",
    "forecast_pair_test_ids = forecast_pair_ids[int(0.9*size):]\n",
    "\n",
    "pair_ids_filt = [pair_id for pair_id in pair_ids if pair_id not in forecast_pair_ids]\n",
    "sim_pair_ids = random.sample(pair_ids_filt, 500)\n",
    "random.shuffle(sim_pair_ids)\n",
    "\n",
    "size = len(sim_pair_ids)\n",
    "sim_pair_train_ids = sim_pair_ids[:int(0.9*size)]\n",
    "sim_pair_val_ids = sim_pair_ids[int(0.9*size):]\n",
    "\n",
    "pair_ids_filt = [pair_id for pair_id in pair_ids if pair_id not in forecast_pair_ids+sim_pair_ids]\n",
    "analysis_pair_ids = random.sample(pair_ids_filt, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa62e56",
   "metadata": {},
   "source": [
    "For each of the conversations in a set, we want to select its corresponding paired conversation and include it in the set as well. Here we can see all the conversations that are included in each selected set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paired_sample(sample):\n",
    "  result = []\n",
    "  for convo_id in sample:\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    result.append(convo)\n",
    "    paired_convo_id = convo.meta['pair_id']\n",
    "    result.append(corpus.get_conversation(paired_convo_id))\n",
    "  return result\n",
    "\n",
    "forecast_train = get_paired_sample(forecast_pair_train_ids)\n",
    "forecast_val = get_paired_sample(forecast_pair_val_ids)\n",
    "forecast_test = get_paired_sample(forecast_pair_test_ids)\n",
    "\n",
    "sim_train = get_paired_sample(sim_pair_train_ids)\n",
    "sim_val = get_paired_sample(sim_pair_val_ids)\n",
    "\n",
    "analysis = get_paired_sample(analysis_pair_ids)\n",
    "\n",
    "print(\"Forecaster (train, val, test)\")\n",
    "print(len(forecast_train), len(forecast_val), len(forecast_test))\n",
    "\n",
    "print(\"Simulator (train, val)\")\n",
    "print(len(sim_train), len(sim_val))\n",
    "\n",
    "print(\"Analysis\")\n",
    "print(len(analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abd6e4",
   "metadata": {},
   "source": [
    "Then, we label each of the conversations with their corresponding split by annotating the conversation metadata field `data_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_split(convos, split):\n",
    "  for convo in convos:\n",
    "    convo.meta[\"data_split\"] = split\n",
    "\n",
    "label_split(corpus.iter_conversations(), None)\n",
    "\n",
    "label_split(forecast_train, \"forecast_train\")\n",
    "label_split(forecast_val, \"forecast_val\")\n",
    "label_split(forecast_test, \"forecast_test\")\n",
    "\n",
    "label_split(sim_train, \"sim_train\")\n",
    "label_split(sim_val, \"sim_val\")\n",
    "\n",
    "label_split(analysis, \"analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f26fba",
   "metadata": {},
   "source": [
    " This function is responsible for creating generic selector functions based on the `data_split` field, where the function returns `True` if the context is included in the corresponding data split. These selector functions will be used to select contexts used for each part of the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6012350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_selector(split):\n",
    "  return lambda context_tuple: context_tuple.current_utterance.get_conversation().meta.get(\"data_split\") == split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872db06",
   "metadata": {},
   "source": [
    "Here, we define selector functions used specifically to fit (train) the forecaster and the simulator models. These are described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecaster_fit_selector(context_tuple, split):\n",
    "  \"\"\"\n",
    "  We use this generic function for both training and validation data.\n",
    "  In both cases, its job is to select only those contexts for which the\n",
    "  FUTURE context is empty. This is in accordance with how CRAFT Model was\n",
    "  originally trained on CGA-CMV, taking the last context from each\n",
    "  conversation (\"last\" defined as being up to and including the chronologically\n",
    "  last utterance as recorded in the corpus)\n",
    "  \"\"\"\n",
    "  matches_split = (context_tuple.current_utterance.get_conversation().meta.get(\"data_split\") == split)\n",
    "  is_end = (len(context_tuple.future_context) == 0)\n",
    "  return (matches_split and is_end)\n",
    "\n",
    "def simulator_fit_selector(context_tuple, split):\n",
    "  \"\"\"\n",
    "  We use this generic function for both training and validation data.\n",
    "  In both cases, its job is to select only those contexts for which the\n",
    "  FUTURE context is not empty, so we have a next utterance to predict.\n",
    "  \"\"\"\n",
    "  matches_split = (context_tuple.current_utterance.get_conversation().meta.get(\"data_split\") == split)\n",
    "  is_end = (len(context_tuple.future_context) == 0)\n",
    "  return (matches_split and not is_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbc023",
   "metadata": {},
   "source": [
    "Our pivotal-moments framework consists of two main components: (1) *simulator model* to simulate potential next responses and (2) *forecaster model* to predict the likelihood of the outcome based on these potential responses. \n",
    "\n",
    "Here, we initialize these two models of types `UtteranceSimulatorModel` and `ForecasterModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3deeba",
   "metadata": {},
   "source": [
    "(1) For the `UtteranceSimulatorModel`, we use `UnslothUtteranceSimulatorModel` which is a general wrapper adapted to the Unsloth framework. Here, we use the 4-bit quantized Llama3-8B model base model, but any model supported by Unsloth can be used. It also possible to load an existing local model by specifying it's saved path. Other models of type `UtteranceSimulatorModel` can be customized and used accordingly.\n",
    "\n",
    "In addition, we could also optionally specify a custom `prompt_fn` that converts contexts to prompts used for the model.\n",
    "\n",
    "We use the following default configs which can be modifed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEFAULT_NUM_SIMULATIONS = 10\n",
    "\n",
    "DEFAULT_LLAMA_CHAT_TEMPLATE = \"llama3\"\n",
    "DEFAULT_LLAMA_CHAT_TEMPLATE_MAPPING = {\n",
    "    \"role\": \"from\",\n",
    "    \"content\": \"value\",\n",
    "    \"user\": \"human\",\n",
    "    \"assistant\": \"gpt\",\n",
    "}\n",
    "\n",
    "DEFAULT_MODEL_CONFIG = {\n",
    "    \"load_in_4bit\": True,\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"dtype\": None,\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"embed_tokens\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0,\n",
    "    \"bias\": \"none\",\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",\n",
    "    \"use_rslora\": False,\n",
    "    \"loftq_config\": None,\n",
    "}\n",
    "\n",
    "DEFAULT_TRAIN_CONFIG = {\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 30,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"eval_steps\": 30,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"logging_steps\": 5,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"output_dir\": \"outputs\",\n",
    "    \"logging_dir\": \"logs\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba48afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "simulator_model = UnslothUtteranceSimulatorModel(\n",
    "  model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
    "  device=DEVICE,\n",
    "  num_simulations=10,\n",
    "  # model_config=DEFAULT_MODEL_CONFIG,\n",
    "  # train_config=DEFAULT_TRAIN_CONFIG,\n",
    "  # chat_template=DEFAULT_LLAMA_CHAT_TEMPLATE,\n",
    "  # chat_template_mapping=DEFAULT_LLAMA_CHAT_TEMPLATE_MAPPING\n",
    "  # prompt_fn=default_prompt_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7602b57",
   "metadata": {},
   "source": [
    "(2) For the `ForecasterModel` in this demo, we use `TransformerEncoderModel` which is a general wrapper adapted to BERT-based forecasting models. Here, we use `roberta-large` for our demo. It also possible to load an existing trained model by specifying it's local path instead. Other forecasting models of type `ForecasterModel` can be used as well.\n",
    "\n",
    "We use the following default config which can be modifed. You can specify your saving directory in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'roberta-large'\n",
    "config_dict = TransformerForecasterConfig(\n",
    "    output_dir= \"YOUR_SAVING_DIRECTORY\",\n",
    "    gradient_accumulation_steps= 1,\n",
    "    per_device_batch_size= 4,\n",
    "    num_train_epochs= 4,\n",
    "    learning_rate= 6.7e-6,\n",
    "    random_seed= 1,\n",
    "    device= DEVICE\n",
    ")\n",
    "forecaster_model = TransformerEncoderModel(model_name_or_path, config=config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac56d6",
   "metadata": {},
   "source": [
    "Here, we now initialize the `PIV` transformer used to compute pivotal scores. The transformer uses the two models of types `UtteranceSimulatorModel` and `ForecasterModel` that we have defined earlier. We also specify metadata fields to save the scores to. Lastly, we add the `labeler` field to indicate the metadata field corresponding to the outcome of the conversation, in this case `has_removed_comment` corresponds to whether the conversation had derailed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c53ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "piv_transformer = PivotalMomentMeasure(\n",
    "  simulator_model=simulator_model,\n",
    "  forecaster_model=forecaster_model,\n",
    "  piv_attribute_name=\"PIV\",\n",
    "  simulated_reply_attribute_name=\"sim_replies\",\n",
    "  simulated_reply_forecast_attribute_name=\"sim_replies_forecasts\",\n",
    "  simulated_reply_forecast_prob_attribute_name=\"sim_replies_forecast_probs\",\n",
    "  forecast_attribute_name=\"forecast\",\n",
    "  forecast_prob_attribute_name=\"forecast_prob\",\n",
    "  labeler=\"has_removed_comment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd43a5",
   "metadata": {},
   "source": [
    "Next, we can fit our transformer. We can individually fit the forecaster and simulator models by calling `fit_forecaster` and `fit_simulator` separately which trains the forecaster and fine-tunes the simulator based on the selected contexts. Alternatively, we can call `fit` to run the whole pipeline. \n",
    "\n",
    "If we want to use an existing trained model, we can skip this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "piv_transformer.fit_forecaster(\n",
    "  corpus=corpus,\n",
    "  train_context_selector=partial(forecaster_fit_selector, split=\"forecast_train\"),\n",
    "  val_context_selector=partial(forecaster_fit_selector, split=\"forecast_val\"),\n",
    "  test_context_selector=make_data_selector(\"forecast_test\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "piv_transformer.fit_simulator(\n",
    "  corpus=corpus,\n",
    "  train_context_selector=partial(simulator_fit_selector, split=\"sim_train\"),\n",
    "  val_context_selector=partial(simulator_fit_selector, split=\"sim_val\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00957670",
   "metadata": {},
   "source": [
    "Now, we have our PIV transformer, we can simply call `transform` to compute pivotal scores on our analysis set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "piv_transformer.transform(\n",
    "  corpus=corpus,\n",
    "  context_selector=make_data_selector(\"analysis\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a11eb0",
   "metadata": {},
   "source": [
    "We can take a look at conversations with their pivotal scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_random_convo(test_convos):\n",
    "  i = random.randint(0, len(test_convos)-1)\n",
    "  convo = test_convos[i]\n",
    "  print(\"has_removed_comment:\", convo.meta[\"has_removed_comment\"])\n",
    "  print()\n",
    "  for ut in convo.iter_utterances():\n",
    "      print(\"[\", round(ut.meta[\"PIV\"], 5), \"]\", ut.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525393e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_random_convo(analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
